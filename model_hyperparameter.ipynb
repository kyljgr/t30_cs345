{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cities and temperature types\n",
    "cities = ['DRESDEN', 'KASSEL', 'DUSSELDORF']\n",
    "temp_types = ['temp_mean', 'temp_max', 'temp_min']\n",
    "\n",
    "# Dictionary to store models\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4031\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 9.821587\n",
      "Mean Squared Error for DRESDEN temp_mean: 2.423859838282691\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4031\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 13.692268\n",
      "Mean Squared Error for DRESDEN temp_max: 3.3841856223763322\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4031\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 5.952241\n",
      "Mean Squared Error for DRESDEN temp_min: 3.8822045531135645\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4047\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 9.577660\n",
      "Mean Squared Error for KASSEL temp_mean: 0.9918063279145789\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4047\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 13.821998\n",
      "Mean Squared Error for KASSEL temp_max: 1.6989309193709086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4047\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 5.584434\n",
      "Mean Squared Error for KASSEL temp_min: 2.428685362637215\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4051\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 11.129217\n",
      "Mean Squared Error for DUSSELDORF temp_mean: 1.3542784192332884\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4051\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 15.299555\n",
      "Mean Squared Error for DUSSELDORF temp_max: 2.072191111472792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4051\n",
      "[LightGBM] [Info] Number of data points in the train set: 2923, number of used features: 24\n",
      "[LightGBM] [Info] Start training from score 6.835854\n",
      "Mean Squared Error for DUSSELDORF temp_min: 2.936228902269142\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    for temp_type in temp_types:\n",
    "        # Prepare features and target\n",
    "        features = data.drop(columns=['DATE'] + [f'{city}_{t}' for t in temp_types])\n",
    "        target = data[f'{city}_{temp_type}']\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "        \n",
    "        grid = [\n",
    "            {'n_estimators': [1, 10, 100, 1000], \n",
    "            'learning_rate': [0.001, 0.01, 0.1, 0.2], \n",
    "            'subsample_for_bin': [200, 2000, 20000, 200000],\n",
    "            'num_leaves': [10, 15, 20, 31, 45]},\n",
    "            ]\n",
    "        \n",
    "        # Create and train the LightGBM model\n",
    "        model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.1, subsample_for_bin=20000, num_leaves=45)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Store the model\n",
    "        models[f'{city}_{temp_type}'] = model\n",
    "        \n",
    "        # Predictions and evaluations\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate and print mean squared error\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        print(f'Mean Squared Error for {city} {temp_type}: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new data to test on custom input\n",
    "new_data = {\n",
    "    'DRESDEN_wind_speed': [3.5],\n",
    "    'DRESDEN_wind_gust': [10.2],\n",
    "    'DRESDEN_humidity': [0.85],\n",
    "    'DRESDEN_global_radiation': [0.15],\n",
    "    'DRESDEN_precipitation': [0.00],\n",
    "    'DRESDEN_sunshine': [5.0],\n",
    "    'DUSSELDORF_temp_mean': [10.0], \n",
    "    'DUSSELDORF_temp_max': [15.0],   \n",
    "    'DUSSELDORF_temp_min': [5.0],   \n",
    "    'DUSSELDORF_wind_speed': [4.0],\n",
    "    'DUSSELDORF_wind_gust': [12.5],\n",
    "    'DUSSELDORF_humidity': [0.80],\n",
    "    'DUSSELDORF_global_radiation': [0.20],\n",
    "    'DUSSELDORF_precipitation': [0.05],\n",
    "    'DUSSELDORF_sunshine': [6.0],\n",
    "    'KASSEL_temp_mean': [9.0],     \n",
    "    'KASSEL_temp_max': [14.0],     \n",
    "    'KASSEL_temp_min': [4.0],   \n",
    "    'KASSEL_wind_speed': [2.8],\n",
    "    'KASSEL_wind_gust': [11.0],\n",
    "    'KASSEL_humidity': [0.88],\n",
    "    'KASSEL_global_radiation': [0.18],\n",
    "    'KASSEL_precipitation': [0.02],\n",
    "    'KASSEL_sunshine': [4.5]\n",
    "}\n",
    "\n",
    "\n",
    "new_features = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted temp_mean for DRESDEN: 9.024635003796599\n",
      "Predicted temp_max for DRESDEN: 13.014866038809078\n",
      "Predicted temp_min for DRESDEN: 6.059938809650521\n",
      "Predicted temp_mean for KASSEL: 3.848618371312498\n",
      "Predicted temp_max for KASSEL: 8.650193518588503\n",
      "Predicted temp_min for KASSEL: 0.5373902165783453\n",
      "Predicted temp_mean for DUSSELDORF: 10.751389483831108\n",
      "Predicted temp_max for DUSSELDORF: 15.624842273030392\n",
      "Predicted temp_min for DUSSELDORF: 5.26486236605847\n"
     ]
    }
   ],
   "source": [
    "# Predict and print results for each city and temperature type\n",
    "for city in cities:\n",
    "    for temp_type in temp_types:\n",
    "        model = models[f'{city}_{temp_type}']  # Retrieve the model\n",
    "        predicted_temp = model.predict(new_features)\n",
    "        print(f\"Predicted {temp_type} for {city}: {predicted_temp[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "<img src=\"Grid.png\" alt=\"Gridsearch values\" width=\"400\"/>\n",
    "\n",
    "We ran the above gridsearch for close to 4 days and it was still running. We were not able to get results from the gridsearch for which parameters were the best estimators so we did our own testing with hyperparameters manually. Here is what we found from doing that:\n",
    "\n",
    "Generally:\n",
    "\n",
    "- test_size closer to 1 creates a larger MSE.\n",
    "\n",
    "- giving max_depth a limit like 3 makes MSE higher\n",
    "\n",
    "- changing boosting_type either gave an error or didn't change the MSE drastically.\n",
    "\n",
    "From the gridsearch values specifically:\n",
    "\n",
    "We are estimating MSE on each city for each temperature type, these tests compare MSE values for Dresden.\n",
    " \n",
    "n_estimators: \n",
    "- 1 -> High MSE, around 30-60\n",
    "- 10 -> Lower MSE but still high, around 7-15\n",
    "- 100 -> (Default) Better MSE, around 2-4\n",
    "- 1000 -> About the same as 100, around 2-4 within ~.03 of the default values\n",
    "- Default and higher performed the best\n",
    "\n",
    "learning_rate:\n",
    "- 0.001 -> High MSE, around 30-60\n",
    "- 0.01 -> Lower MSE but still high, around 5-15\n",
    "- 0.1 -> (Default) Better MSE, around 2-4\n",
    "- 0.2 -> Slightly higher MSE than with 0.1\n",
    "- The default performed the best out of these values\n",
    "\n",
    "subsample_for_bin:\n",
    "- 200 -> Similar to default MSE values for previous parameters, around 2-4\n",
    "- 2000 -> Same as with 200, slightly different values but within 0.2\n",
    "- 20000 -> Same as above\n",
    "- 200000 -> (Default) Same as above\n",
    "- All performed about the same  \n",
    "\n",
    "num_leaves\n",
    "- 10 -> Lower MSE, around 2-4\n",
    "- 15 -> Same as above\n",
    "- 20 -> Same as above\n",
    "- 31 -> (Default) Values of MSE around 2-4\n",
    "- 45 -> Slightly lower than default values\n",
    "- Higher number of leaves performed better, going higher than 45 stayed at about the same values\n",
    "\n",
    "\n",
    "From this we tried <img src=\"Regressor.png\" alt=\"Regressor parameters\" width=\"600\"/>\n",
    "\n",
    "And we got these values for the Dresden temperature observations and the MSE values:\n",
    "\n",
    "<img src=\"MSE_Values.png\" alt=\"MSE values\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "**The default values for the most part worked the best for these parameters in the LightGBM regressor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
